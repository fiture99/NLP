{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import bcolz  # to process the data from Glove File \n",
    "import pickle # to dump and load pretrained glove vectors \n",
    "import copy   # to make deepcopy of python lists and dictionaries\n",
    "import operator\n",
    "import numpy as np\n",
    "from pandas import DataFrame # to visualize the glove word embeddings in form of DataFrame\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "idx = 0\n",
    "word2idx = {}\n",
    "vectors = bcolz.carray(np.zeros(1), rootdir=f'data/6B.300.dat', mode='w')\n",
    "\n",
    "with open(f'data/glove.6B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        words.append(word)\n",
    "        word2idx[word] = idx\n",
    "        idx += 1\n",
    "        vect = np.array(line[1:]).astype(np.float)\n",
    "        vectors.append(vect)\n",
    "    \n",
    "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), rootdir=f'data/6B.300.dat', mode='w')\n",
    "vectors.flush()\n",
    "pickle.dump(words, open(f'data/6B.300_words.pkl', 'wb'))\n",
    "pickle.dump(word2idx, open(f'data/6B.300_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = bcolz.open(f'data/6B.300.dat')[:]\n",
    "words = pickle.load(open(f'data/6B.300_words.pkl', 'rb'))\n",
    "word2idx = pickle.load(open(f'data/6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dframe = DataFrame(vectors, columns=range(1, 301), index=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>-0.245610</td>\n",
       "      <td>0.068010</td>\n",
       "      <td>0.182540</td>\n",
       "      <td>-0.295510</td>\n",
       "      <td>0.018007</td>\n",
       "      <td>0.168110</td>\n",
       "      <td>0.191090</td>\n",
       "      <td>0.063650</td>\n",
       "      <td>0.212430</td>\n",
       "      <td>-2.1165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095983</td>\n",
       "      <td>-0.347390</td>\n",
       "      <td>0.032640</td>\n",
       "      <td>0.047579</td>\n",
       "      <td>0.222420</td>\n",
       "      <td>-0.231400</td>\n",
       "      <td>0.114310</td>\n",
       "      <td>-0.669140</td>\n",
       "      <td>-0.034753</td>\n",
       "      <td>0.097869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>them</th>\n",
       "      <td>-0.205880</td>\n",
       "      <td>0.192570</td>\n",
       "      <td>-0.118270</td>\n",
       "      <td>-0.157120</td>\n",
       "      <td>-0.380160</td>\n",
       "      <td>0.059840</td>\n",
       "      <td>-0.032788</td>\n",
       "      <td>0.174650</td>\n",
       "      <td>0.403970</td>\n",
       "      <td>-1.8908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057513</td>\n",
       "      <td>-0.650600</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>-0.135320</td>\n",
       "      <td>0.115990</td>\n",
       "      <td>-0.244490</td>\n",
       "      <td>0.048469</td>\n",
       "      <td>-0.328680</td>\n",
       "      <td>-0.169600</td>\n",
       "      <td>-0.129650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>-0.200170</td>\n",
       "      <td>0.143020</td>\n",
       "      <td>0.052055</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>0.014899</td>\n",
       "      <td>-0.255240</td>\n",
       "      <td>-0.179070</td>\n",
       "      <td>-0.046713</td>\n",
       "      <td>-2.0547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045239</td>\n",
       "      <td>-0.352980</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.281040</td>\n",
       "      <td>0.203380</td>\n",
       "      <td>-0.478800</td>\n",
       "      <td>-0.039697</td>\n",
       "      <td>0.034939</td>\n",
       "      <td>-0.125990</td>\n",
       "      <td>0.218630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>him</th>\n",
       "      <td>-0.158840</td>\n",
       "      <td>0.360950</td>\n",
       "      <td>-0.113370</td>\n",
       "      <td>-0.421430</td>\n",
       "      <td>-0.284790</td>\n",
       "      <td>-0.089365</td>\n",
       "      <td>0.125030</td>\n",
       "      <td>-0.359220</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>-1.6332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030342</td>\n",
       "      <td>-0.196140</td>\n",
       "      <td>-0.160160</td>\n",
       "      <td>-0.285130</td>\n",
       "      <td>0.277510</td>\n",
       "      <td>-0.143610</td>\n",
       "      <td>0.214780</td>\n",
       "      <td>-0.223810</td>\n",
       "      <td>-0.293430</td>\n",
       "      <td>-0.033526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>united</th>\n",
       "      <td>0.056277</td>\n",
       "      <td>-0.165980</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>-0.064785</td>\n",
       "      <td>0.268810</td>\n",
       "      <td>0.108790</td>\n",
       "      <td>-0.807370</td>\n",
       "      <td>0.384470</td>\n",
       "      <td>0.128930</td>\n",
       "      <td>-1.5534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017244</td>\n",
       "      <td>-0.120950</td>\n",
       "      <td>0.501220</td>\n",
       "      <td>0.161380</td>\n",
       "      <td>0.359710</td>\n",
       "      <td>0.752880</td>\n",
       "      <td>0.479690</td>\n",
       "      <td>-0.274290</td>\n",
       "      <td>-0.171260</td>\n",
       "      <td>-0.803720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>during</th>\n",
       "      <td>-0.039635</td>\n",
       "      <td>0.212610</td>\n",
       "      <td>-0.245140</td>\n",
       "      <td>-0.266630</td>\n",
       "      <td>0.189180</td>\n",
       "      <td>0.099316</td>\n",
       "      <td>-0.370910</td>\n",
       "      <td>0.038365</td>\n",
       "      <td>-0.092625</td>\n",
       "      <td>-1.6565</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148390</td>\n",
       "      <td>-0.020586</td>\n",
       "      <td>-0.697330</td>\n",
       "      <td>0.248950</td>\n",
       "      <td>0.214960</td>\n",
       "      <td>0.351270</td>\n",
       "      <td>-0.146670</td>\n",
       "      <td>-0.142210</td>\n",
       "      <td>-0.464110</td>\n",
       "      <td>-0.159580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>before</th>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.109870</td>\n",
       "      <td>0.063975</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.100660</td>\n",
       "      <td>0.045820</td>\n",
       "      <td>-0.178650</td>\n",
       "      <td>-0.146560</td>\n",
       "      <td>0.223870</td>\n",
       "      <td>-1.5851</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124490</td>\n",
       "      <td>-0.147260</td>\n",
       "      <td>0.078155</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>0.012701</td>\n",
       "      <td>0.176650</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>-0.481250</td>\n",
       "      <td>-0.078685</td>\n",
       "      <td>0.200680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>-0.376040</td>\n",
       "      <td>0.241160</td>\n",
       "      <td>-0.260980</td>\n",
       "      <td>-0.007960</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.098515</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>0.137970</td>\n",
       "      <td>0.110260</td>\n",
       "      <td>-1.7503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083795</td>\n",
       "      <td>-0.158100</td>\n",
       "      <td>0.325670</td>\n",
       "      <td>-0.429300</td>\n",
       "      <td>0.110570</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>-0.255790</td>\n",
       "      <td>-0.757310</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>-0.081830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>since</th>\n",
       "      <td>0.132110</td>\n",
       "      <td>0.009039</td>\n",
       "      <td>0.464310</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.289240</td>\n",
       "      <td>-0.014632</td>\n",
       "      <td>-0.309260</td>\n",
       "      <td>-0.102920</td>\n",
       "      <td>0.319730</td>\n",
       "      <td>-2.0459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104320</td>\n",
       "      <td>-0.210160</td>\n",
       "      <td>0.261070</td>\n",
       "      <td>0.085519</td>\n",
       "      <td>0.171470</td>\n",
       "      <td>0.854160</td>\n",
       "      <td>0.069214</td>\n",
       "      <td>-0.649110</td>\n",
       "      <td>0.410390</td>\n",
       "      <td>0.080164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>-0.155070</td>\n",
       "      <td>0.284220</td>\n",
       "      <td>0.311910</td>\n",
       "      <td>-0.195960</td>\n",
       "      <td>0.125730</td>\n",
       "      <td>-0.152090</td>\n",
       "      <td>0.145480</td>\n",
       "      <td>0.337110</td>\n",
       "      <td>-0.093931</td>\n",
       "      <td>-1.4419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124250</td>\n",
       "      <td>-0.531690</td>\n",
       "      <td>-0.037670</td>\n",
       "      <td>0.150680</td>\n",
       "      <td>-0.139130</td>\n",
       "      <td>-0.088808</td>\n",
       "      <td>-0.227130</td>\n",
       "      <td>-0.545620</td>\n",
       "      <td>-0.147350</td>\n",
       "      <td>-0.314930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7    \\\n",
       "so     -0.245610  0.068010  0.182540 -0.295510  0.018007  0.168110  0.191090   \n",
       "them   -0.205880  0.192570 -0.118270 -0.157120 -0.380160  0.059840 -0.032788   \n",
       "what   -0.200170  0.143020  0.052055 -0.000809  0.017009  0.014899 -0.255240   \n",
       "him    -0.158840  0.360950 -0.113370 -0.421430 -0.284790 -0.089365  0.125030   \n",
       "united  0.056277 -0.165980  0.332410 -0.064785  0.268810  0.108790 -0.807370   \n",
       "during -0.039635  0.212610 -0.245140 -0.266630  0.189180  0.099316 -0.370910   \n",
       "before  0.000730  0.109870  0.063975  0.026729  0.100660  0.045820 -0.178650   \n",
       "may    -0.376040  0.241160 -0.260980 -0.007960  0.219800  0.098515  0.234580   \n",
       "since   0.132110  0.009039  0.464310  0.160600  0.289240 -0.014632 -0.309260   \n",
       "many   -0.155070  0.284220  0.311910 -0.195960  0.125730 -0.152090  0.145480   \n",
       "\n",
       "             8         9       10   ...       291       292       293  \\\n",
       "so      0.063650  0.212430 -2.1165  ...  0.095983 -0.347390  0.032640   \n",
       "them    0.174650  0.403970 -1.8908  ... -0.057513 -0.650600  0.028065   \n",
       "what   -0.179070 -0.046713 -2.0547  ...  0.045239 -0.352980  0.333500   \n",
       "him    -0.359220  0.027257 -1.6332  ...  0.030342 -0.196140 -0.160160   \n",
       "united  0.384470  0.128930 -1.5534  ...  0.017244 -0.120950  0.501220   \n",
       "during  0.038365 -0.092625 -1.6565  ... -0.148390 -0.020586 -0.697330   \n",
       "before -0.146560  0.223870 -1.5851  ...  0.124490 -0.147260  0.078155   \n",
       "may     0.137970  0.110260 -1.7503  ... -0.083795 -0.158100  0.325670   \n",
       "since  -0.102920  0.319730 -2.0459  ...  0.104320 -0.210160  0.261070   \n",
       "many    0.337110 -0.093931 -1.4419  ... -0.124250 -0.531690 -0.037670   \n",
       "\n",
       "             294       295       296       297       298       299       300  \n",
       "so      0.047579  0.222420 -0.231400  0.114310 -0.669140 -0.034753  0.097869  \n",
       "them   -0.135320  0.115990 -0.244490  0.048469 -0.328680 -0.169600 -0.129650  \n",
       "what    0.281040  0.203380 -0.478800 -0.039697  0.034939 -0.125990  0.218630  \n",
       "him    -0.285130  0.277510 -0.143610  0.214780 -0.223810 -0.293430 -0.033526  \n",
       "united  0.161380  0.359710  0.752880  0.479690 -0.274290 -0.171260 -0.803720  \n",
       "during  0.248950  0.214960  0.351270 -0.146670 -0.142210 -0.464110 -0.159580  \n",
       "before -0.052333  0.012701  0.176650  0.020622 -0.481250 -0.078685  0.200680  \n",
       "may    -0.429300  0.110570  0.312300 -0.255790 -0.757310  0.097391 -0.081830  \n",
       "since   0.085519  0.171470  0.854160  0.069214 -0.649110  0.410390  0.080164  \n",
       "many    0.150680 -0.139130 -0.088808 -0.227130 -0.545620 -0.147350 -0.314930  \n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dframe[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_index = word2idx['sos']\n",
    "eos_index = word2idx['eos']\n",
    "sos_swap_word = words[0]\n",
    "eos_swap_word = words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0], words[sos_index] = words[sos_index], words[0]\n",
    "words[1], words[eos_index] = words[eos_index], words[1]\n",
    "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
    "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
    "        self.word2count = { word : 1 for word in words }\n",
    "        self.index2word = { i : word for word, i in word2idx.items() }\n",
    "        self.n_words = 400001\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class OutputLang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2,reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/eng-wol.txt', encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "   \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = InputLang(\"eng\")\n",
    "        output_lang = OutputLang(\"wol\")\n",
    "    else:\n",
    "        input_lang = InputLang(\"eng\")\n",
    "        output_lang = OutputLang(\"wol\")\n",
    "    \n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 60\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 3687 sentence pairs\n",
      "Trimmed to 3687 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 400100\n",
      "wol 2218\n",
      "['one', 'benna']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'wol', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = input_lang.n_words\n",
    "\n",
    "weights_matrix = np.zeros((matrix_len, 300))\n",
    "# words_found = 0\n",
    "miss = []\n",
    "for i, word in enumerate(input_lang.word2index):\n",
    "        try: \n",
    "            weights_matrix[i] = glove[word]\n",
    "#             words_found += 1\n",
    "        except KeyError: miss.append(word)\n",
    "#             weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 39s (- 449m 27s) (5000 5%) 4.0845\n",
      "50m 52s (- 457m 54s) (10000 10%) 3.9835\n",
      "72m 49s (- 412m 37s) (15000 15%) 3.9167\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.01).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1,100000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1246a8370>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"hello lamin\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = where does lamin leave\n",
      "output = <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-e847184b7ee5>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "<ipython-input-55-e847184b7ee5>:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "<ipython-input-55-e847184b7ee5>:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"where does lamin leave\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = what is your name ?\n",
      "output = <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-e847184b7ee5>:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "<ipython-input-55-e847184b7ee5>:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n",
      "<ipython-input-55-e847184b7ee5>:17: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"what is your name ?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miss = []\n",
    "# words=''\n",
    "# for words in (input_lang.word2index):\n",
    "#     try:evaluateAndShowAttention(words)\n",
    "#     except KeyError: miss.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"export.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(AttnDecoderRNN, PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e916adfd5b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
